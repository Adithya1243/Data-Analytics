{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61fadc8",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this project, I decided to scrape metadata of two popular youtube channels that I like  - Pewdiepie and Mr.Beast. These channels have a very high subscriber count and also put out a variety of interesting videos. Using the video metadata, I wanted to see how the viewing trends have changed over the past few years as it is influenced directly by like count, view count and also on video duration.\n",
    "\n",
    "After identifying some research questions and analysing them, I wanted to create a machine learning model to assess the popularity of a new video based on the view count, comment count and the video duration.\n",
    "\n",
    "I have utilised the youtube API to scrape required data, and I secured the below details. \n",
    "\n",
    "## Column Descriptions\n",
    "\n",
    "- videoId - A unique video id\n",
    "- title\t- The title of the youtube video\n",
    "- publishedAt - The date of upload\n",
    "- categoryId - A number which categorises the video type. (refer https://mixedanalytics.com/blog/list-of-youtube-video-category-ids/ for more information)\n",
    "- categoryName\t- Name of the category\n",
    "- tags - Common tags associated with the youtube video\n",
    "- viewCount\t- The total number of views till date\n",
    "- likeCount\t- The total number of likes till date\n",
    "- dislikeCount\t- Should ideally show the dislike count, but was removed in latest youtube policy updates.\n",
    "- commentCount\t- Total number of comments for the youtube video\n",
    "- duration - The duration of the youtube video\n",
    "- subscriberCount - Total number of subscribers till date.\n",
    "\n",
    "Here the youtube api only provided the latest subscriber count and not the historical count when the video was published. And due to the latest policy update the dislike count is zero.\n",
    "\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "Here are some of the research questions I have identified\n",
    "\n",
    "-  What are the yearly trends in video view counts?\n",
    "- What is the general correlation between the like count and view count across the dataset of YouTube videos, and what does the scatter plot reveal about their relationship?\n",
    "- What are the top 10 tags observed in all the youtube videos\n",
    "- How does the plot for Average like v/s Video duration look like\n",
    "- What is the average duration of videos for PewDiePie and MrBeast, and how does video length relate to popularity metrics like views and likes?\n",
    "\n",
    "\n",
    "# 2. Data Collection\n",
    "\n",
    "A Google cloud account is needed, where a new project is created and 'YouTube Data API v3' is enabled. After creating the credentials a unique api key is available to pull publicly accessible data. There was another api (Youtube Analytics API) which could provide deeper insights(demographics details of subscribers) of a youtube channel, but was only available to its owner. The data used in this project was only the publicly available data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b375c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424d3b1",
   "metadata": {},
   "source": [
    "I start by gathering video IDs from a specific YouTube channel. My code first retrieves the channel's ID by using the channel's username provided to the YouTube API. Once I have the channel ID, I use it to fetch the IDs of videos uploaded by this channel. I've set up my code to perform this action twice(which can be adjusted to pull more data), each time collecting up to 50 video IDs per request, while making sure not to include any video IDs that I've already collected previously. This helps in building a comprehensive list of videos for further analysis without redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297ffe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_video_ids(channel_username, api_key, existing_ids):\n",
    "    base_url = \"https://www.googleapis.com/youtube/v3/\"\n",
    "    channel_endpoint = base_url + \"channels\"\n",
    "    channel_params = {\n",
    "        'part': 'id',\n",
    "        'forUsername': channel_username,\n",
    "        'key': api_key\n",
    "    }\n",
    "    channel_response = requests.get(channel_endpoint, params=channel_params).json()\n",
    "\n",
    "    # Check for errors in the response\n",
    "    if 'error' in channel_response:\n",
    "        print(f\"Error in response: {channel_response['error']}\")\n",
    "        return []\n",
    "\n",
    "    if 'items' not in channel_response or not channel_response['items']:\n",
    "        print(f\"No items found for channel: {channel_username}. Response: {channel_response}\")\n",
    "        return []\n",
    "\n",
    "    channel_id = channel_response['items'][0]['id']\n",
    "    \n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "    for _ in range(2):  # Adjust the range as needed\n",
    "        search_endpoint = base_url + \"search\"\n",
    "        search_params = {\n",
    "            'part': 'id',\n",
    "            'channelId': channel_id,\n",
    "            'type': 'video',\n",
    "            'maxResults': 50,\n",
    "            'pageToken': next_page_token,\n",
    "            'key': api_key\n",
    "        }\n",
    "        search_response = requests.get(search_endpoint, params=search_params).json()\n",
    "        if 'items' in search_response:\n",
    "            for item in search_response['items']:\n",
    "                video_id = item['id']['videoId']\n",
    "                if video_id not in existing_ids:\n",
    "                    video_ids.append(video_id)\n",
    "            next_page_token = search_response.get('nextPageToken', None)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return video_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c9b6a",
   "metadata": {},
   "source": [
    "Next, I focus on collecting detailed information about each video. For each video ID obtained previously, I use the YouTube API to gather various pieces of data. This includes the video's title, publication date, category, and various engagement metrics like view count, like count, and so on. I also fetch channel-related data to add more context to the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f9866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_youtube_data(video_id, api_key):\n",
    "    video_url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "    channel_url = \"https://www.googleapis.com/youtube/v3/channels\"\n",
    "    category_url = \"https://www.googleapis.com/youtube/v3/videoCategories\"\n",
    "    \n",
    "    # Fetch video data\n",
    "    video_params = {\n",
    "        'part': 'snippet,statistics,contentDetails',\n",
    "        'id': video_id,\n",
    "        'key': api_key\n",
    "    }\n",
    "    video_response = requests.get(video_url, params=video_params).json()\n",
    "    video_data = video_response['items'][0]\n",
    "    \n",
    "    # Fetch channel data\n",
    "    channel_id = video_data['snippet']['channelId']\n",
    "    channel_params = {\n",
    "        'part': 'statistics',\n",
    "        'id': channel_id,\n",
    "        'key': api_key\n",
    "    }\n",
    "    channel_response = requests.get(channel_url, params=channel_params).json()\n",
    "    channel_data = channel_response['items'][0]\n",
    "    \n",
    "    # Fetch category name\n",
    "    category_id = video_data['snippet']['categoryId']\n",
    "    category_params = {\n",
    "        'part': 'snippet',\n",
    "        'id': category_id,\n",
    "        'key': api_key\n",
    "    }\n",
    "    category_response = requests.get(category_url, params=category_params).json()\n",
    "    category_name = category_response['items'][0]['snippet']['title']\n",
    "    \n",
    "    # Compile and return the data\n",
    "    data = {\n",
    "        'videoId': video_id,  \n",
    "        'title': video_data['snippet'].get('title', None),\n",
    "        'publishedAt': video_data['snippet'].get('publishedAt', None),\n",
    "        'categoryId': video_data['snippet'].get('categoryId', None),\n",
    "        'categoryName': category_name,\n",
    "        'tags': video_data['snippet'].get('tags', []),\n",
    "        'viewCount': video_data['statistics'].get('viewCount', 0),\n",
    "        'likeCount': video_data['statistics'].get('likeCount', 0),\n",
    "        'dislikeCount': video_data['statistics'].get('dislikeCount', 0),\n",
    "        'commentCount': video_data['statistics'].get('commentCount', 0),\n",
    "        'duration': video_data['contentDetails'].get('duration', None),\n",
    "        'subscriberCount': channel_data['statistics'].get('subscriberCount', 0)\n",
    "    }\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854da58c",
   "metadata": {},
   "source": [
    "Here, I've written a function to handle potential quota errors from the YouTube API. Since there's a limit to the number of requests one can make, this function helps in identifying when these limits are exceeded, allowing me to avoid unnecessary errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf7a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if quota limit error is in the response\n",
    "def is_quota_error(response):\n",
    "    if 'error' in response:\n",
    "        error_code = response['error']['code']\n",
    "        if error_code == 403:\n",
    "            error_reason = response['error']['errors'][0]['reason']\n",
    "            if error_reason == 'quotaExceeded' or error_reason == 'rateLimitExceeded':\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab233d2",
   "metadata": {},
   "source": [
    "In the below code chunk, a new csv file is created or an existing one is used to build on the previous data. I have setup appropriate print statements to keep track of whats happening. Here, \"pewdiepie.csv\" is for storing PewDiePie channel content and \"mrbeast.csv\" for MrBeast's data. I have introduced this variation as I exceeded my daily limit in scraping just one creator's content per day. This was the workaround for getting more data.\n",
    "\n",
    "The channel usernames are defined in a variable called channel_username, which has to be uncommented to pull its respective data. The rest of the code in the below chunk calls the previously defined functions to get unique video id's which are further used to get video metadata, and another check for the daily limit is performed to ensure smooth execution of the code. The scraped data is then converted to a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a003c68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp2.csv not found. A new file will be created.\n"
     ]
    }
   ],
   "source": [
    "filename = \"pewdiepie.csv\"  # for pewdiepie data\n",
    "# filename = \"mrbeast.csv\" # for mrbeast data\n",
    "\n",
    "# Load existing data or create an empty DataFrame\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    existing_ids = set(df['videoId'].tolist())\n",
    "    print(f\"Loaded existing data from {filename}.\")\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame()\n",
    "    existing_ids = set()\n",
    "    print(f\"{filename} not found. A new file will be created.\")\n",
    "\n",
    "# Defining channel names\n",
    "channel_username = ['pewdiepie'] # Enabling this line by default to scrape PewDiePie's data\n",
    "# channel_username = ['mrbeast6000'] # uncomment this line to scrape MrBeast data\n",
    "\n",
    "# API key\n",
    "api_key = 'AIzaSyBmEK-PE9FQzdqO14kx-NyOyh_PNrq4B9k'\n",
    "\n",
    "# fetching the video id's\n",
    "video_ids = fetch_video_ids(channel_username, api_key, existing_ids)\n",
    "\n",
    "# fetching the metadata of each video \n",
    "for vid in video_ids:\n",
    "    video_data = fetch_youtube_data(vid, api_key)\n",
    "    \n",
    "    # to check if the daily API limit is reached\n",
    "    if is_quota_error(video_data):\n",
    "        print(\"YouTube API quota limit reached. Stopping data fetch.\")\n",
    "        break\n",
    "        \n",
    "    # converting the data to a dataframe   \n",
    "    df_new = pd.DataFrame([video_data])\n",
    "    df = pd.concat([df, df_new], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9ac7a",
   "metadata": {},
   "source": [
    "### Saving data to csv file\n",
    "\n",
    "A total of 642 rows for PewDiepie and 597 rows for MrBeast is scraped so far and saved to their respective csv files using the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8cd046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data updated and saved to temp2.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving data to csv file\n",
    "df.to_csv(filename, index=False, encoding='utf-8')\n",
    "print(f\"Data updated and saved to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
